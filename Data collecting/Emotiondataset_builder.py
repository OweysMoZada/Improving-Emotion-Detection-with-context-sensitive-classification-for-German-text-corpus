import pandas as pd
import numpy as np

import csv
from itertools import chain
import re
from tqdm import tqdm

import urllib.request as urllib2
import requests
import codecs
import warnings
from deep_translator import GoogleTranslator

from nltk import tokenize
import nltk
nltk.download('punkt')


def url_encode(word):
    word = codecs.encode(word,'utf-8')    
    url_Word = urllib2.quote(word)
    
    return url_Word


def translate_texts(sentence):
    """
        A function used to translate sentences.
    """
    warnings.warn("Internetverbindung muss erhalten bleiben. No Error handling")
    
    return GoogleTranslator(source='en', target='de').translate(sentence)


def generate_example_text_based_on_keywords(word:str, label:str, limit=80, sentence_len=12):
    """
    A function used to generate sentences based on one (or more) words

    ...

    Attributes
    ----------
    word : str
        give a word(s) to get  the sentence the word(s) appears in
        
    label : str
        give a label to the word, which will be provided through output
        
    limit : int
        set a number of sentences being generated by the API (default=80)
        
    sentence_len : str
        length of sentence of a generated sentence by the API (default=12)
    """
            
    # call api to get sentences
    word_url = url_encode(word)
    url = "https://www.dwds.de/r/?q="+word_url+"&limit="+str(limit)+"&view=tsv"
    df = pd.read_csv(url,sep='\t',encoding = "utf-8")
    
    #if word does not exist in corpus
    if df.Hit.isnull()[0] == True:
        return pd.DataFrame({'label':[np.nan], 'text_de':[np.nan]})
    
    df = df[df['Hit'].str.split().str.len().lt(sentence_len)]
    
    # split sentences into unnested lists
    df = df.Hit.map(tokenize.sent_tokenize)
    df = list(chain.from_iterable(df))
    
    # just get the sentence the word appears in
    df = [s for s in df if any(xs in s for xs in [word])]
    
    #make df with corresponding emotion 
    df = pd.DataFrame({'label':len(df)*[label], 
                       'text_de':df}) 
    
    if df.empty:
        df = df.append([np.nan], ignore_index=True)
        return df
        
    df = del_negation(df)
    
    return df


def del_negation(df):
    df = df.dropna()
    df.reset_index(drop=True, inplace=True)
    df['text_de'] = df['text_de'].str.lower()
    
    df = df[~df['text_de'].str.contains('nicht')]
    df = df[~df['text_de'].str.contains('ohne')]
    df = df[~df['text_de'].str.contains('keine')]
    df = df[~df['text_de'].str.contains('kein')]
    df = df[~df['text_de'].str.contains('keinen')]
    df = df[~df['text_de'].str.contains('keinem')]
    df = df[~df['text_de'].str.contains('keiner')]
    
    return df